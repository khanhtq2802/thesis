{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model_path = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "# sentiment_analysis_pipe = pipeline(\n",
    "#     \"sentiment-analysis\", \n",
    "#     model=model_path, \n",
    "#     tokenizer=model_path, \n",
    "#     device=0 if torch.cuda.is_available() else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Path to your CSV file\n",
    "filename = \"data/reviews.csv\"\n",
    "\n",
    "# Empty list to store the first column\n",
    "first_column = []\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(filename, 'r') as csvfile:\n",
    "  # Create a CSV reader object\n",
    "  csv_reader = csv.reader(csvfile)\n",
    "\n",
    "  # Skip the header row (optional, adjust based on your CSV)\n",
    "  next(csv_reader, None)\n",
    "\n",
    "  # Iterate through each row\n",
    "  for row in csv_reader:\n",
    "    # Extract the first element (assuming comma delimiter)\n",
    "    first_column.append(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We visited the Citadel with great expectations. It is very large, with a good collection of black n white photos of their kings and society hundreds of years ago.\\nWe were most disappointed to see a flat empty open space where the Forbidden City once stood - bombed and destroyed completely during the war. Much of the rest of the imperial complex had also been similarly destroyed. Restoration work had recreated some spaces but much more needs to be done.\\nWith a good guide, we were able to appreciate the highlights of its history.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_column[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/khanh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')  # Download sentence tokenizer if not already installed\n",
    "\n",
    "def paragraphs_to_sentences(paragraphs):\n",
    "  \"\"\"\n",
    "  This function takes a list of paragraphs (strings) and returns a list of sentences (strings).\n",
    "\n",
    "  Args:\n",
    "      paragraphs: A list of strings, where each string represents a paragraph.\n",
    "\n",
    "  Returns:\n",
    "      A list of strings, where each string represents a sentence.\n",
    "  \"\"\"\n",
    "  sentences = []\n",
    "  for paragraph in paragraphs:\n",
    "    sentences.extend(nltk.sent_tokenize(paragraph))\n",
    "  return sentences\n",
    "\n",
    "sentences = paragraphs_to_sentences(first_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29648\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We visited the Citadel with great expectations.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'negative', 'score': 0.723576545715332}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment_analysis_pipe(\"Covid cases are increasing fast!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9739198088645935}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentiment_analysis_pipe(sentences[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(sentiment_analysis_pipe(sentences[:1])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "texts = sentences\n",
    "\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "# PT\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "with open('output.csv', 'w', newline='') as csvfile:\n",
    "   fieldnames = ['text', 'positive_score', 'negative_score', 'neutral_score']\n",
    "   writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "   writer.writeheader()\n",
    "\n",
    "   for text in texts:\n",
    "       encoded_input = tokenizer(text, return_tensors='pt')\n",
    "       output = model(**encoded_input)\n",
    "       scores = output[0][0].detach().numpy()\n",
    "       scores = softmax(scores)\n",
    "\n",
    "       positive_score = scores[config.label2id['positive']]\n",
    "       negative_score = scores[config.label2id['negative']]\n",
    "       neutral_score = scores[config.label2id['neutral']]\n",
    "\n",
    "       writer.writerow({\n",
    "           'text': text,\n",
    "           'positive_score': positive_score,\n",
    "           'negative_score': negative_score,\n",
    "           'neutral_score': neutral_score\n",
    "       })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
